import torch
import torch.nn as nn
import os
from collections import OrderedDict

from models.pointnet import PointNetFeatureExtractor

class DilatedTCNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dilation, dropout_p=0.2):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Dropout(dropout_p),
            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(),
            nn.Dropout(dropout_p)
        )
        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.downsample(x)

class KeyframeRegressor(nn.Module):
    def __init__(self, input_dim_1d, point_feature_dim, num_channels, dropout_p, num_keyframes=6):
        super().__init__()
       

        self.point_cloud_extractor = PointNetFeatureExtractor()
        model_weights_path = "./pointnet_adapted.pth"

        try:
            if os.path.exists(model_weights_path):
                print(f"åŠ è½½ PointNet++ é¢„è®­ç»ƒæƒé‡: {model_weights_path}")
                full_model_state_dict = torch.load(model_weights_path, map_location='cpu')
                extractor_weights = OrderedDict()
                for key, value in full_model_state_dict.items():
                    if key.startswith("point_cloud_extractor."):
                        new_key = key[len("point_cloud_extractor."):]
                        extractor_weights[new_key] = value
                if len(extractor_weights) == 0:
                    print("âš ï¸ æ²¡æ‰¾åˆ°æå–å™¨æƒé‡ï¼Œå°è¯•ç›´æ¥åŠ è½½")
                    self.point_cloud_extractor.load_state_dict(full_model_state_dict, strict=False)
                else:
                    self.point_cloud_extractor.load_state_dict(extractor_weights, strict=True)
                    print("âœ… æˆåŠŸåŠ è½½ PointNet++ é¢„è®­ç»ƒç‰¹å¾æå–å™¨")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        except Exception as e:
            print(f"åŠ è½½ PointNet++ è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

        print("ğŸ”’ å†»ç»“ PointNetFeatureExtractor æƒé‡")
        for param in self.point_cloud_extractor.parameters():
            param.requires_grad = False

        self.point_cloud_extractor.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

        # TCN encoder
        fused_input_dim = input_dim_1d + point_feature_dim
        layers = []
        for i, out_ch in enumerate(num_channels):
            dilation = 2 ** i
            in_ch = fused_input_dim if i == 0 else num_channels[i - 1]
            layers.append(DilatedTCNBlock(in_ch, out_ch, dilation, dropout_p=dropout_p))
        self.encoder = nn.Sequential(*layers)

        self.pooling = nn.AdaptiveAvgPool1d(1)
        self.regression_head = nn.Sequential(
            nn.Linear(num_channels[-1], 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_keyframes)
        )

    def forward(self, x, point_cloud, src_key_padding_mask=None):
        # x: [B, T, D_1d]
        # point_cloud: [B, T, N, 3]  (æ¯å¸§çš„ç‚¹äº‘)

        # æå–æ¯ä¸€å¸§çš„ç‚¹äº‘ç‰¹å¾ï¼ˆé€å¸§æå–ï¼‰
        B, T, N, _ = point_cloud.shape
        point_cloud = point_cloud.view(B * T, N, 3)  # [B*T, N, 3]
        point_features = self.point_cloud_extractor(point_cloud)  # [B*T, D_pc]
        point_features = point_features.view(B, T, -1)  # [B, T, D_pc]

        # æ‹¼æ¥ 1D ç‰¹å¾ä¸ç‚¹äº‘ç‰¹å¾
        fused = torch.cat([x, point_features], dim=-1)  # [B, T, D_1d + D_pc]
        if src_key_padding_mask is not None:
            # maskæ˜¯Trueçš„åœ°æ–¹æ˜¯paddingï¼Œæ‰€ä»¥æˆ‘ä»¬è¦æŠŠè¿™äº›åœ°æ–¹çš„ç‰¹å¾ç½®ä¸º0
            # unsqueeze(-1) æ˜¯ä¸ºäº†è®©maskèƒ½å’Œå¤šç»´ç‰¹å¾è¿›è¡Œå¹¿æ’­æ“ä½œ
            fused = fused.masked_fill(src_key_padding_mask.unsqueeze(-1), 0.0)
        fused = fused.permute(0, 2, 1)  # [B, D, T] for Conv1d

        # ç¼–ç å™¨ + pooling + regression
        embedding = self.encoder(fused)  # [B, D_out, T]
        global_feature = self.pooling(embedding).squeeze(-1)  # [B, D_out]
        predicted_coords_logits = self.regression_head(global_feature)  # [B, num_keyframes]
        predicted_coords_normalized = torch.sigmoid(predicted_coords_logits)  # [0,1] èŒƒå›´

        return predicted_coords_normalized
